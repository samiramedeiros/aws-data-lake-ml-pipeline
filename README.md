# ğŸ¦ Banking Data Lake & Analytics Pipeline (AWS)

### ğŸ“Œ VisÃ£o Geral

Este projeto implementa um **pipeline completo de Engenharia de Dados na AWS**, simulando um cenÃ¡rio real do setor bancÃ¡rio.  
O foco estÃ¡ na **construÃ§Ã£o de um Data Lake bem estruturado**, com camadas bem definidas, automaÃ§Ã£o via **AWS Step Functions** e consumo analÃ­tico via **Amazon Athena**.

O projeto foi pensado como um **case profissional**, priorizando boas prÃ¡ticas de arquitetura, clareza de responsabilidades entre camadas e governanÃ§a de dados.

---

### ğŸ¯ Objetivos do Projeto

- Construir um **Data Lake na AWS** seguindo o padrÃ£o:
  - **Raw â†’ Processed â†’ Curated**
- Processar dados transacionais bancÃ¡rios com **AWS Glue (Spark)**
- Gerar **datasets analÃ­ticos prontos para negÃ³cio e ML**
- Orquestrar todo o fluxo com **AWS Step Functions**
- Disponibilizar os dados finais para consulta via **Amazon Athena**

---

### ğŸ§± Arquitetura do Data Lake

### ğŸ“‚ Camadas

### ğŸ”¹ Raw/Bronze
- Dados brutos de transaÃ§Ãµes bancÃ¡rias
- Sem tratamento
- Armazenados no Amazon S3

### ğŸ”¹ Processed/Silver
- Dados limpos e padronizados
- Tipos ajustados, colunas normalizadas
- Prontos para agregaÃ§Ãµes e regras de negÃ³cio

### ğŸ”¹ Curated/Gold (Camada AnalÃ­tica)
A camada **Curated NÃƒO contÃ©m dados transacionais crus**.  
Ela Ã© composta apenas por **datasets analÃ­ticos e mÃ©tricas**, como:

- `customer_insights` â†’ mÃ©tricas por cliente
- `risk_analysis` â†’ indicadores de risco
- `ml_features` â†’ features prontas para Machine Learning

> O curated layer Ã© voltado para consumo analÃ­tico e modelos. TransaÃ§Ãµes cruas ficam no processed para evitar duplicaÃ§Ã£o e custos desnecessÃ¡rios. 

---

### ğŸ”„ Fluxo do Pipeline

1. **Raw â†’ Processed**
   - Job Glue: `raw_to_processed.py`
   - Limpeza e padronizaÃ§Ã£o dos dados

2. **Processed â†’ Curated**
   - Job Glue: `processed_to_curated.py`
   - GeraÃ§Ã£o de mÃ©tricas por cliente

3. **Processed â†’ Risk Analysis**
   - Job Glue: `processed_to_risk_analysis.py`
   - CriaÃ§Ã£o de indicadores de risco

4. **Processed â†’ ML Features**
   - Job Glue: `processed_to_ml_features.py`
   - GeraÃ§Ã£o de features prontas para modelos

5. **OrquestraÃ§Ã£o**
   - AWS Step Functions coordena toda a execuÃ§Ã£o diÃ¡ria

6. **Consumo**
   - Amazon Athena consulta os dados curados

---

### â© AWS Step Functions

<img width="519" height="495" alt="step-functions" src="https://github.com/user-attachments/assets/5539f398-64a2-4eae-850c-1f759d957564" />


---

### ğŸ§  Onde entra Machine Learning?

Este projeto **nÃ£o treina modelos de Machine Learning propositalmente**.

### âœ”ï¸ DecisÃ£o arquitetural consciente 

O papel da Engenharia de Dados neste contexto Ã©:

- Garantir **dados confiÃ¡veis**
- Criar **features reutilizÃ¡veis**
- Preparar dados para **cientistas de dados ou pipelines de ML**

O dataset `ml_features` representa exatamente o **contrato entre Engenharia de Dados e Machine Learning**.

> Em ambientes reais, o treinamento de modelos ocorre em pipelines separados (ex: SageMaker), consumindo essas features.

---

### ğŸ§° Tecnologias Utilizadas

- **AWS S3** â€” Data Lake
- **AWS Glue (Spark)** â€” Processamento de dados
- **AWS Step Functions** â€” OrquestraÃ§Ã£o
- **Amazon Athena** â€” Consultas analÃ­ticas
- **Python / PySpark**
- **Apache Parquet**

---

### ğŸ“ Estrutura do RepositÃ³rio

```text
aws-data-lake-ml-pipeline/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ glue_jobs/
â”‚   â”‚   â”œâ”€â”€ raw_to_processed.py
â”‚   â”‚   â”œâ”€â”€ processed_to_curated.py
â”‚   â”‚   â”œâ”€â”€ processed_to_risk_analysis.py
â”‚   â”‚   â””â”€â”€ processed_to_ml_features.py
â”‚   â””â”€â”€ step_functions/
â”‚       â””â”€â”€ daily_pipeline.json
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

### ğŸ“Š Exemplos de Uso (Athena)
```
SELECT COUNT(*) FROM banking_db.customer_insights;
SELECT COUNT(*) FROM banking_db.risk_analysis;
SELECT COUNT(*) FROM banking_db.ml_features;
```

---

### ğŸš€ ConclusÃ£o

Este projeto demonstra:

- Arquitetura de Data Lake madura

- SeparaÃ§Ã£o clara de responsabilidades

- Boas prÃ¡ticas de Engenharia de Dados

- VisÃ£o realista de como ML se integra ao pipeline

- AutomaÃ§Ã£o e escalabilidade na AWS

---

### ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja `LICENSE` para mais detalhes.

---

### ğŸ‘¤ Autor

### **Samira Medeiros**
- GitHub: [Samira Medeiros](https://github.com/samiramedeiros)
- LinkedIn: [Samira Medeiros](https://www.linkedin.com/in/samiramedeirosc)
- Email: [samiramedeirosc@email.com](mailto:samiramedeirosc@email.com)
